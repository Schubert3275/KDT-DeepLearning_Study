{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import fetch_openml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KDP-25\\.conda\\envs\\Torch_PY38\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "mnist_fashion = fetch_openml('Fashion-MNIST', as_frame=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 784), (70000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = mnist_fashion.data, mnist_fashion.target\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAOI0lEQVR4nO3cv2/V9dvH8ev096+kFFJ+CRK/6MCAMQaIMGo0YuLA7urErIkO/gXuLkZd0WicjIGEQYkGf0TioAaNVENKsWCAtpS2tOfertz3/f0m7fVOWojfx2Pm5ed4esqTs1ydbrfbDQCIiJ4H/QIAeHiIAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKAKS+B/0CYD3dbre86XQ6m/BK/t3c3Fx5c+HChaZnnTx5smlX1fJ+r66uljd9ff+8v35a3rtWm/UZ900BgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgDpn3eRin+ctbW18qa3t7e8+e2338qbd999t7wZHh4ubyIiRkdHy5uhoaHy5tixY+XNVh63azk61/IZannOVr4PLUcIN/J74ZsCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSg3g89Dbr8Nf/d/78+fLm3Llz5c3+/fvLm4iIpaWl8ubu3bvlzdmzZ8ubV199tbzZtWtXeRMR0el0ypuWz0OL+fn5pl1PT/3f5yMjI03PWo9vCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASA7i8dAbGBjYkud8++235c3U1FR5s7a2Vt607l544YXy5ocffihvXn/99fLmyJEj5U1ExOHDh8ubQ4cOlTfffPNNedPyGYqIOHHiRHlz/Pjx8mZ8fHzdP+ObAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAUqfb7XYf9Ivgv0PrR63T6ZQ3586dK29ajrrdunWrvOnv7y9vIiJ6erbm33BHjx4tbx5//PHypvXQYcvnaGZmprzp66vfCz127Fh5ExHx0UcflTenT58ub5599tl1/4xvCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQHIllebrpVul5UrqM888U95MTU2VNy1a3+/e3t7yZnBwsOlZVUNDQ+VNy881IuLpp58ub5544onypuX9/vzzz8ubiIjff/+9vJmenm561np8UwAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQOp70C+AB6/1MNnDbGJiory5du1aeTM8PFzeLC0tlTcRESsrK+XN/Px8edNy3G5xcbG8af3cXbhwobz56quvypuWw4XXr18vbyIiXnzxxabdZvBNAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAyUE8/pHu3r1b3qyurpY3a2tr5U3LEb2IiN27d5c3O3bsKG+mpqbKm56e+r8vWw7ORbT9nFoO9rX8P/X29pY3ERFXr15t2m0G3xQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJAcxKPpMFnLIbjWY2Hz8/PlzfT0dHkzODhY3gwMDJQ3y8vL5U1E2+sbHR0tb27fvl3etBzeazlaGNH2/o2NjZU3d+7cKW8OHz5c3kRELCwslDffffddeXPkyJF1/4xvCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQHIlleh0OuXN6upqedN6JfXMmTPlzbVr18qbycnJ8mZxcbG8aX0fWi5p/vnnn+VNf39/ebO0tFTe9PW1/fWzsrJS3rT8nG7cuFHenD59uryJiLh06VJ5c//+/aZnrcc3BQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoApE632+0+6BfBg9VyWKv1mFmLixcvljcvvfRSeTM8PFzebOVhwPn5+fJmaGiovNm+fXt50/IZajlsF9F2GHBiYqLpWVUt73dExGuvvVbevPLKK03PWo9vCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASFt31WyDWu/ztRwmW1tbK29aXl9/f39509Ozdb3eyuN2LU6ePFnejI2NlTctB/GWl5fLm1aTk5PlTcuhunv37pU3AwMD5U2rls9ry+9Ty98pP/74Y3kTETE+Pt602wy+KQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIG3qJbSWg1K9vb1Nz3rYj7o9zL744ovy5uOPPy5vLly4UN5ERIyMjJQ3O3bsKG+WlpbKm06nU960flZb3oeW38GW96HliF7LexcRMTo62rSrajl22PraPvnkk/Lm5ZdfbnrWenxTACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBA6nS73e6DfhEPyt9//13eTE9PlzeXL1/ekudEtB3Wanl9g4OD5c3a2lp5ExExMDBQ3iwuLpY3e/fuLW9ajqatrKyUNxERN27cKG9afk53794tb06cOFHezM3NlTcREV9++WV509NT//fv+Ph4edPyeYiI2L17d3nz888/Nz1rPb4pAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAaVOvpH799dflzVtvvdX0rNnZ2fLm1q1b5U3LtcWW66Dbtm0rbyIient7y5uWq5gt1zdbP2rDw8PlzaFDh8qbM2fOlDdHjx4tb+7cuVPeRLR9XqemppqeVfXYY4+VN/Pz803PGhsbK29GR0fLm5bfi4WFhfImIuL27dvlTcsl4I3wTQGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAGnDB/FWV1fL//Hjx4+XN9PT0+VNRERfX19503LcruWwVov79+837VqOx22VlqNfERE3b94sbz744IPy5uzZs+XNO++8U97s2bOnvImIGBoaKm9aDtUdPHiwvPn111/Lm5afa0REf39/edPy+9RyuHBlZaW8iWg7ZPnHH380PWs9vikAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACBt+CDee++9V/6Pv/HGG+XNv/71r/ImImJhYaG8mZubK2+WlpbKmxatB/Fajs7t27evvHnkkUfKm9nZ2fImImJtba28mZmZKW8+/fTT8ubevXvlzZUrV8qbiLbP+Pfff78lm5aDmYODg+VNRNvnYXl5uelZVRv86/TftLy+ixcvljf79+9f98/4pgBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgNS30T+4c+fO8n+85dBay5G6iLbjWo8++mh50/L6VlZWyps7d+6UNxER27dvL28OHDhQ3rS8D0NDQ+VN6663t7e8OXXqVHlz+PDh8mZqaqq8iYi4efNmedPye7Ft27bypr+/v7xp+RlFRAwMDJQ3LQfnenrq/2ZuPYjXsrt8+XJ54yAeACWiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQNnwQr+W4XctBqY0cbPpPFhYWypvZ2dnypuVY2OTk5JZsIiLu379f3iwtLW3Jc+7du1feRETMz8+XN6urq+XNjh07ypuffvqpvBkbGytvItoOOE5MTJQ3LT+nls9rX9+G//r5P1qO77U8a3FxsbyZmZkpbyIixsfHy5tLly6VN88999y6f8Y3BQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIG34dOBTTz1V/o+fOnWqvHn//ffLm4iIvXv3ljcHDx4sb4aGhsqbliufy8vL5U1E22XHlZWV8qblSmrLe9f6rE6nU96MjIyUN3v27ClvWq4HR0T09vaWNy3vXcsl4Lm5ufJmcHCwvIloe30tm4GBgfKm5YJrRMSVK1fKm127djU9az2+KQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIHW63W73Qb+I/+2zzz5r2r399tvlzV9//VXeTE5Oljctx7haj6atra2VN0tLS+XN6upqedNynC0iouUj2nIQr+X1tRwubD122PL6turXu+U5O3fu3IRX8p+1HH1s+R2cmZkpbyIinnzyyfLmww8/bHrWenxTACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBA2vBBvJZDa61H3bbK+fPny5s333yzvLl+/Xp5c/v27fImou0wWctxu5YDY319feVNxNYdW2s5ordv377ypvX3YmxsrLxp+dlulYGBgabdyMhIedPy99fzzz9f3hw6dKi8iYg4ceJE024zPNx/awOwpUQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACBt+CAeW+uXX35p2s3OzpY3ExMT5c3Vq1fLmwMHDpQ3EW2H0w4ePNj0LPhv55sCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQXEkFIPmmAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQ/ge0qfPOeiUN1wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_fashion(image_data):\n",
    "    image = image_data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = 'binary')\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "some_fashion = X[0]\n",
    "plot_fashion(some_fashion)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 784]),\n",
       " torch.Size([60000]),\n",
       " torch.Size([10000, 784]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.as_tensor(X[:60000], dtype=torch.float32)\n",
    "y_train = torch.as_tensor(y[:60000], dtype=torch.int64)\n",
    "X_test = torch.as_tensor(X[60000:], dtype=torch.float32)\n",
    "y_test = torch.as_tensor(y[60000:], dtype=torch.int64)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = X_train / 255.0\n",
    "scaled_X_test = X_test / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[9],\n",
      "        [0],\n",
      "        [0],\n",
      "        ...,\n",
      "        [3],\n",
      "        [0],\n",
      "        [5]])\n"
     ]
    }
   ],
   "source": [
    "y_one_hot = torch.zeros(y_train.shape[0], 10)\n",
    "print(y_one_hot)\n",
    "print(y_train.unsqueeze(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([60000, 10])\n"
     ]
    }
   ],
   "source": [
    "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "print(y_one_hot)\n",
    "print(y_one_hot.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(784, 10)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/10000 Cost: 2.418640\n",
      "Epoch  100/10000 Cost: 1.335700\n",
      "Epoch  200/10000 Cost: 1.072670\n",
      "Epoch  300/10000 Cost: 0.953813\n",
      "Epoch  400/10000 Cost: 0.883368\n",
      "Epoch  500/10000 Cost: 0.835196\n",
      "Epoch  600/10000 Cost: 0.799345\n",
      "Epoch  700/10000 Cost: 0.771168\n",
      "Epoch  800/10000 Cost: 0.748175\n",
      "Epoch  900/10000 Cost: 0.728895\n",
      "Epoch 1000/10000 Cost: 0.712393\n",
      "Epoch 1100/10000 Cost: 0.698039\n",
      "Epoch 1200/10000 Cost: 0.685393\n",
      "Epoch 1300/10000 Cost: 0.674131\n",
      "Epoch 1400/10000 Cost: 0.664012\n",
      "Epoch 1500/10000 Cost: 0.654851\n",
      "Epoch 1600/10000 Cost: 0.646503\n",
      "Epoch 1700/10000 Cost: 0.638851\n",
      "Epoch 1800/10000 Cost: 0.631803\n",
      "Epoch 1900/10000 Cost: 0.625282\n",
      "Epoch 2000/10000 Cost: 0.619224\n",
      "Epoch 2100/10000 Cost: 0.613576\n",
      "Epoch 2200/10000 Cost: 0.608292\n",
      "Epoch 2300/10000 Cost: 0.603336\n",
      "Epoch 2400/10000 Cost: 0.598673\n",
      "Epoch 2500/10000 Cost: 0.594276\n",
      "Epoch 2600/10000 Cost: 0.590119\n",
      "Epoch 2700/10000 Cost: 0.586182\n",
      "Epoch 2800/10000 Cost: 0.582446\n",
      "Epoch 2900/10000 Cost: 0.578894\n",
      "Epoch 3000/10000 Cost: 0.575510\n",
      "Epoch 3100/10000 Cost: 0.572283\n",
      "Epoch 3200/10000 Cost: 0.569200\n",
      "Epoch 3300/10000 Cost: 0.566250\n",
      "Epoch 3400/10000 Cost: 0.563424\n",
      "Epoch 3500/10000 Cost: 0.560715\n",
      "Epoch 3600/10000 Cost: 0.558112\n",
      "Epoch 3700/10000 Cost: 0.555611\n",
      "Epoch 3800/10000 Cost: 0.553204\n",
      "Epoch 3900/10000 Cost: 0.550885\n",
      "Epoch 4000/10000 Cost: 0.548650\n",
      "Epoch 4100/10000 Cost: 0.546493\n",
      "Epoch 4200/10000 Cost: 0.544409\n",
      "Epoch 4300/10000 Cost: 0.542395\n",
      "Epoch 4400/10000 Cost: 0.540447\n",
      "Epoch 4500/10000 Cost: 0.538561\n",
      "Epoch 4600/10000 Cost: 0.536733\n",
      "Epoch 4700/10000 Cost: 0.534962\n",
      "Epoch 4800/10000 Cost: 0.533243\n",
      "Epoch 4900/10000 Cost: 0.531575\n",
      "Epoch 5000/10000 Cost: 0.529955\n",
      "Epoch 5100/10000 Cost: 0.528380\n",
      "Epoch 5200/10000 Cost: 0.526848\n",
      "Epoch 5300/10000 Cost: 0.525359\n",
      "Epoch 5400/10000 Cost: 0.523908\n",
      "Epoch 5500/10000 Cost: 0.522496\n",
      "Epoch 5600/10000 Cost: 0.521120\n",
      "Epoch 5700/10000 Cost: 0.519778\n",
      "Epoch 5800/10000 Cost: 0.518469\n",
      "Epoch 5900/10000 Cost: 0.517193\n",
      "Epoch 6000/10000 Cost: 0.515946\n",
      "Epoch 6100/10000 Cost: 0.514729\n",
      "Epoch 6200/10000 Cost: 0.513540\n",
      "Epoch 6300/10000 Cost: 0.512378\n",
      "Epoch 6400/10000 Cost: 0.511243\n",
      "Epoch 6500/10000 Cost: 0.510132\n",
      "Epoch 6600/10000 Cost: 0.509045\n",
      "Epoch 6700/10000 Cost: 0.507981\n",
      "Epoch 6800/10000 Cost: 0.506940\n",
      "Epoch 6900/10000 Cost: 0.505920\n",
      "Epoch 7000/10000 Cost: 0.504921\n",
      "Epoch 7100/10000 Cost: 0.503942\n",
      "Epoch 7200/10000 Cost: 0.502983\n",
      "Epoch 7300/10000 Cost: 0.502043\n",
      "Epoch 7400/10000 Cost: 0.501120\n",
      "Epoch 7500/10000 Cost: 0.500215\n",
      "Epoch 7600/10000 Cost: 0.499327\n",
      "Epoch 7700/10000 Cost: 0.498456\n",
      "Epoch 7800/10000 Cost: 0.497600\n",
      "Epoch 7900/10000 Cost: 0.496760\n",
      "Epoch 8000/10000 Cost: 0.495935\n",
      "Epoch 8100/10000 Cost: 0.495124\n",
      "Epoch 8200/10000 Cost: 0.494327\n",
      "Epoch 8300/10000 Cost: 0.493544\n",
      "Epoch 8400/10000 Cost: 0.492775\n",
      "Epoch 8500/10000 Cost: 0.492018\n",
      "Epoch 8600/10000 Cost: 0.491274\n",
      "Epoch 8700/10000 Cost: 0.490541\n",
      "Epoch 8800/10000 Cost: 0.489821\n",
      "Epoch 8900/10000 Cost: 0.489112\n",
      "Epoch 9000/10000 Cost: 0.488415\n",
      "Epoch 9100/10000 Cost: 0.487728\n",
      "Epoch 9200/10000 Cost: 0.487052\n",
      "Epoch 9300/10000 Cost: 0.486386\n",
      "Epoch 9400/10000 Cost: 0.485730\n",
      "Epoch 9500/10000 Cost: 0.485085\n",
      "Epoch 9600/10000 Cost: 0.484448\n",
      "Epoch 9700/10000 Cost: 0.483821\n",
      "Epoch 9800/10000 Cost: 0.483203\n",
      "Epoch 9900/10000 Cost: 0.482594\n",
      "Epoch 10000/10000 Cost: 0.481994\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 10000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    prediction = model(scaled_X_train)\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch:4d}/{nb_epochs} Cost: {cost.item():.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KDP-25\\AppData\\Local\\Temp\\ipykernel_9288\\701917176.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pre_y = F.softmax(model(scaled_X_test[i])).max(0)[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8271\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for i in range(scaled_X_test.shape[0]):\n",
    "    pre_y = F.softmax(model(scaled_X_test[i])).max(0)[1]\n",
    "    if pre_y == y_test[i]:\n",
    "        num += 1\n",
    "print(num/scaled_X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_PY38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
