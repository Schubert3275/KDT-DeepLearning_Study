{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & DataLoader 살펴보기\n",
    "\n",
    "-   Pytorch에서 배치 크기만큼 데이터를 조절하기 위한 메카니즘\n",
    "-   Dataset : 사용 데이터를 기반으로 사용자 정의 클래스 작성\n",
    "-   DataLoader : 지정된 Dataset에서 지정된 batch size만큼 피쳐와 타겟을 추출하여 전달\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1] 모듈 로딩 및 데이터 준비 <hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15.2a0\n"
     ]
    }
   ],
   "source": [
    "### 모듈 로딩\n",
    "# import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torchvision\n",
    "\n",
    "print(torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data = > torch.Size([5, 3]), 2D\n",
      "y_data = > torch.Size([5, 1]), 2D\n"
     ]
    }
   ],
   "source": [
    "### 데이터 준비\n",
    "\n",
    "x_data = torch.IntTensor(\n",
    "    [[10, 20, 30], [20, 30, 40], [30, 40, 50], [40, 50, 60], [50, 60, 70]]\n",
    ")\n",
    "y_data = torch.IntTensor([[20], [30], [40], [50], [60]])\n",
    "\n",
    "print(f\"x_data = > {x_data.shape}, {x_data.ndim}D\")\n",
    "print(f\"y_data = > {y_data.shape}, {y_data.ndim}D\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2] 데이터셋 생성 <hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2-1] TensorDataset 활용 : Dataset의 sub_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorDataset 클래스 로딩\n",
    "from torch.utils.data import TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x17ba4df04c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TensorDataset(\n",
    "    x_data, y_data\n",
    ")  # x_data, y_data shape[0]이 다르면 dataset 생성 안 됨\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10, 20, 30],\n",
       "         [20, 30, 40],\n",
       "         [30, 40, 50],\n",
       "         [40, 50, 60],\n",
       "         [50, 60, 70]], dtype=torch.int32),\n",
       " tensor([[20],\n",
       "         [30],\n",
       "         [40],\n",
       "         [50],\n",
       "         [60]], dtype=torch.int32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10, 20, 30], dtype=torch.int32), tensor([20], dtype=torch.int32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### __getitem__() 메서드 호출\n",
    "dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2-2] 사용자정의 데이터셋 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   species       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n"
     ]
    }
   ],
   "source": [
    "### 데이터 준비\n",
    "filename = \"../data/text/iris.csv\"\n",
    "\n",
    "irisDF = pd.read_csv(filename)\n",
    "irisDF.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisNP = np.loadtxt(filename, delimiter=\",\", skiprows=1, usecols=(0, 1, 2, 3))\n",
    "irisNP[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pandas.core.frame.DataFrame, numpy.ndarray, 'DataFrame', 'ndarray')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 데이터의 타입 체크\n",
    "type(irisDF), type(irisNP), irisDF.__class__.__name__, irisNP.__class__.__name__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF\n"
     ]
    }
   ],
   "source": [
    "if irisDF.__class__.__name__ == \"DataFrame\":\n",
    "    print(\"DF\")\n",
    "else:\n",
    "    print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False, True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(irisDF, pd.DataFrame), isinstance(irisNP, pd.DataFrame), isinstance(\n",
    "    irisNP, np.ndarray\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance([10], list), isinstance({\"A\": 22}, list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 사용자정의 DataSet 클래스\n",
    "# - 데이터의 Tensor 변환\n",
    "class DLDataset(Dataset):\n",
    "    # 초기화 콜백함수(callback function)\n",
    "    def __init__(self, x_data, y_data):\n",
    "        super().__init__()\n",
    "        # x,y 데이터 ==> ndarray\n",
    "        x_data = x_data.values if isinstance(x_data, pd.DataFrame) else x_data\n",
    "        y_data = y_data.values if isinstance(y_data, pd.DataFrame) else y_data\n",
    "\n",
    "        # ndarray ==> tensor\n",
    "        self.feature = torch.FloatTensor(x_data)\n",
    "        self.target = torch.LongTensor(y_data).squeeze()\n",
    "\n",
    "    # 데이터셋의 갯수 체크 콜백함수(callback function)\n",
    "    def __len__(self):\n",
    "        return self.target.shape[0]\n",
    "\n",
    "    # 특정 인덱스 데이터 + 라벨 반환 콜백함수(callback function)\n",
    "    def __getitem__(self, index):\n",
    "        return self.feature[index], self.target[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featureDF => (150, 4), 2D\n",
      "targetDF  => (150,), 1D\n"
     ]
    }
   ],
   "source": [
    "## 피쳐와 라벨로 분리\n",
    "featureDF = irisDF[irisDF.columns[:-1]]\n",
    "targetDF = irisDF[irisDF.columns[-1]]\n",
    "\n",
    "print(f\"featureDF => {featureDF.shape}, {featureDF.ndim}D\")\n",
    "print(f\"targetDF  => {targetDF.shape}, {targetDF.ndim}D\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 1) 2\n"
     ]
    }
   ],
   "source": [
    "# object 타입 타겟 ===> int 타입 타겟 변환\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "targetNP = LabelEncoder().fit_transform(targetDF)\n",
    "targetNP = targetNP.reshape(-1, 1)\n",
    "\n",
    "print(targetNP.shape, targetNP.ndim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성 -> DF, NP\n",
    "my_dataset = DLDataset(featureDF, targetNP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([5.1000, 3.5000, 1.4000, 0.2000]), tensor(0)),\n",
       " sepal_length    5.1\n",
       " sepal_width     3.5\n",
       " petal_length    1.4\n",
       " petal_width     0.2\n",
       " Name: 0, dtype: float64,\n",
       " 'setosa')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset[0], featureDF.iloc[0], targetDF[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5.1000, 3.5000, 1.4000, 0.2000]), tensor(0))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 생성 -> NP, NP\n",
    "my_dataset2 = DLDataset(irisNP, targetNP)\n",
    "my_dataset2[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   [2-3] 학습용, 검증용 테스트용 Dataset <hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainDS => 105개, validDS => 15개, testDS => 30개\n",
      "Subset 속성 =>\n",
      "indices : [42, 95, 30, 64, 52, 35, 130, 40, 82, 17, 108, 94, 68, 97, 117, 127, 41, 44, 57, 140, 149, 32, 23, 102, 16, 113, 71, 18, 67, 66, 0, 25, 101, 112, 91, 3, 59, 116, 86, 84, 106, 142, 43, 39, 26, 98, 93, 20, 87, 19, 120, 114, 7, 63, 76, 89, 36, 45, 37, 56, 58, 122, 51, 145, 24, 21, 105, 62, 15, 11, 48, 133, 88, 50, 6, 134, 111, 8, 49, 75, 69, 124, 4, 147, 80, 100, 99, 141, 47, 107, 13, 109, 129, 28, 38, 53, 121, 5, 55, 31, 73, 74, 54, 29, 12]\n",
      "dataset : <__main__.DLDataset object at 0x0000017BC2D44BE0>\n",
      "Subset 속성 =>\n",
      "indices : [22, 104, 81, 1, 103, 125, 85, 2, 96, 128, 27, 118, 77, 110, 146]\n",
      "dataset : <__main__.DLDataset object at 0x0000017BC2D44BE0>\n"
     ]
    }
   ],
   "source": [
    "### ===> 파이토치\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# 학습용, 검증용, 테스트 데이터 비율\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "\n",
    "trainDS, validDS, testDS = random_split(my_dataset2, [0.7, 0.1, 0.2], generator=seed)\n",
    "\n",
    "print(\n",
    "    f\"trainDS => {len(trainDS)}개, validDS => {len(validDS)}개, testDS => {len(testDS)}개\"\n",
    ")\n",
    "\n",
    "print(f\"Subset 속성 =>\\nindices : {trainDS.indices}\\ndataset : {trainDS.dataset}\")\n",
    "print(f\"Subset 속성 =>\\nindices : {validDS.indices}\\ndataset : {validDS.dataset}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] DataLoader 생성 : 학습용, 검증용, 테스트용 <hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "# drop_last 매개변수 : 배치사이즈로 데이터셋 분리 후 남는 데이터 처리 방법 설정 [기본 : False]\n",
    "batch = 5\n",
    "trainDL = DataLoader(trainDS, batch_size=batch)\n",
    "validDL = DataLoader(validDS, batch_size=batch)\n",
    "testDL = DataLoader(testDS, batch_size=batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size : 5\n",
      "trainDS => 105개, validDS => 15개, testDS => 30개\n",
      "trainDL => 21개, validDL => 3개, testDL => 6개\n"
     ]
    }
   ],
   "source": [
    "# Epoch당 반복 단위\n",
    "print(f\"batch_size : {batch}\")\n",
    "print(\n",
    "    f\"trainDS => {len(trainDS)}개, validDS => {len(validDS)}개, testDS => {len(testDS)}개\"\n",
    ")\n",
    "print(\n",
    "    f\"trainDL => {len(trainDL)}개, validDL => {len(validDL)}개, testDL => {len(testDL)}개\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] Model 클래스 정의 : 입/출력 피쳐 수, 층 수, 은닉층의 노드수 <hr>\n",
    "\n",
    "-   구조 설계\n",
    "    -   입력측 : 입력 <= 피쳐 갯수, iris 4개\n",
    "    -   은닉층 : 마음대로 알아서 잘\n",
    "    -   출력측 : 출력 <= [분류] 타겟 클래스 갯수, [회귀] 1개\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 클래스 정의\n",
    "# 클래스명 : CModel\n",
    "class CModel(nn.Module):\n",
    "    # 구성요소 정의 함수\n",
    "    def __init__(self, in_, out_):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(in_, 100)\n",
    "        self.hidden_layer = nn.Linear(100, 27)\n",
    "        self.output_layer = nn.Linear(27, out_)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    # 순방향 학습 진행 함수\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)  # W1X1+W2X2+...+WnXn+b 100개 반환\n",
    "        x = self.relu(x)  # relu() 결과 100개 반환\n",
    "        x = self.hidden_layer(x)  # W1X1+W2X2+...+WnXn+b 27개 반환\n",
    "        x = self.relu(x)  # relu() 결과 27개 반환\n",
    "        x = self.output_layer(x)  # W1X1+W2X2+...+WnXn+b out_개 반환\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] 학습 준비 : 실행디바이스, 모델, 최적화, 손실함수, 학습횟수, 학습함수, 평가함수, 예측함수 <hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행 디바이스 설정\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 학습 횟수\n",
    "EPOCHS = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN: 4, OUT: 3\n",
      "CModel(\n",
      "  (input_layer): Linear(in_features=4, out_features=100, bias=True)\n",
      "  (hidden_layer): Linear(in_features=100, out_features=27, bias=True)\n",
      "  (output_layer): Linear(in_features=27, out_features=3, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스\n",
    "IN, OUT = my_dataset2.feature.shape[1], len(torch.unique(my_dataset2.target))\n",
    "model = CModel(IN, OUT).to(DEVICE)\n",
    "print(f\"IN: {IN}, OUT: {OUT}\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수\n",
    "LOSS_FN = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "# 최적화 인스턴스\n",
    "OPTIMIZER = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   학습 및 검증관련 함수 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 학습 진행함수\n",
    "def training():\n",
    "    # 학습모드 => 정규화, 경사하강법, 드랍아웃 등의 기능을 활성화\n",
    "    model.train()\n",
    "\n",
    "    # 배치크기만큼 학습 진행 및 저장\n",
    "    train_loss = []\n",
    "    for cnt, (feature, target) in enumerate(trainDL):\n",
    "        # 배치크가만큼의 학습 데이터 준비\n",
    "        feature, target = feature.to(DEVICE), target.to(DEVICE)\n",
    "\n",
    "        # 학습\n",
    "        pre_target = model(feature)\n",
    "        # print(f'pre_target => {pre_target.shape}, {pre_target.ndim}D')\n",
    "        # print(f'target     => {target.shape}, {target.ndim}D')\n",
    "\n",
    "        # 손실계산\n",
    "        loss = LOSS_FN(pre_target, target)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        # W,b 업데이트\n",
    "        OPTIMIZER.zero_grad()\n",
    "        loss.backward()\n",
    "        OPTIMIZER.step()\n",
    "\n",
    "        # 배치 단위 학습 진행 메세지 출력\n",
    "        # print(f\"[Train {cnt} batch Loss] ==> {loss.item():.4f}\")\n",
    "\n",
    "    # 에포크 단위 학습 진행 메세지 출력\n",
    "    print(f\"[Train loss] ==> {loss.item():.4f}\")\n",
    "\n",
    "    return train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 검증 및 평가 진행함수\n",
    "def testing():\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ===> 예측 함수\n",
    "def predict():\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6] 학습 진행 <hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train loss] ==> 0.0005\n",
      "[0/50] 0.006206681532646707\n",
      "[Train loss] ==> 0.0004\n",
      "[1/50] 0.006115017592535531\n",
      "[Train loss] ==> 0.0005\n",
      "[2/50] 0.005850160969809319\n",
      "[Train loss] ==> 0.0004\n",
      "[3/50] 0.005841752132601826\n",
      "[Train loss] ==> 0.0004\n",
      "[4/50] 0.005832664227671644\n",
      "[Train loss] ==> 0.0004\n",
      "[5/50] 0.005809915737440211\n",
      "[Train loss] ==> 0.0004\n",
      "[6/50] 0.005426515055573657\n",
      "[Train loss] ==> 0.0004\n",
      "[7/50] 0.005437404360106614\n",
      "[Train loss] ==> 0.0004\n",
      "[8/50] 0.005266017439550653\n",
      "[Train loss] ==> 0.0003\n",
      "[9/50] 0.005628965044403837\n",
      "[Train loss] ==> 0.0004\n",
      "[10/50] 0.004992682599376643\n",
      "[Train loss] ==> 0.0003\n",
      "[11/50] 0.005111400021546399\n",
      "[Train loss] ==> 0.0003\n",
      "[12/50] 0.005047437883366088\n",
      "[Train loss] ==> 0.0003\n",
      "[13/50] 0.004962520392187538\n",
      "[Train loss] ==> 0.0003\n",
      "[14/50] 0.0048018172760099075\n",
      "[Train loss] ==> 0.0003\n",
      "[15/50] 0.004455418613620817\n",
      "[Train loss] ==> 0.0003\n",
      "[16/50] 0.004658917128576182\n",
      "[Train loss] ==> 0.0003\n",
      "[17/50] 0.0044017177601095425\n",
      "[Train loss] ==> 0.0003\n",
      "[18/50] 0.004549553498128219\n",
      "[Train loss] ==> 0.0003\n",
      "[19/50] 0.004524703594316158\n",
      "[Train loss] ==> 0.0003\n",
      "[20/50] 0.004403349239943228\n",
      "[Train loss] ==> 0.0003\n",
      "[21/50] 0.004259599051216529\n",
      "[Train loss] ==> 0.0003\n",
      "[22/50] 0.003960453794726553\n",
      "[Train loss] ==> 0.0002\n",
      "[23/50] 0.004202995491227955\n",
      "[Train loss] ==> 0.0002\n",
      "[24/50] 0.003965122523368336\n",
      "[Train loss] ==> 0.0003\n",
      "[25/50] 0.0040824781056774855\n",
      "[Train loss] ==> 0.0002\n",
      "[26/50] 0.0041141259806906985\n",
      "[Train loss] ==> 0.0002\n",
      "[27/50] 0.0038834037312924573\n",
      "[Train loss] ==> 0.0002\n",
      "[28/50] 0.003836267458003325\n",
      "[Train loss] ==> 0.0002\n",
      "[29/50] 0.003670986003618011\n",
      "[Train loss] ==> 0.0002\n",
      "[30/50] 0.003691168716527283\n",
      "[Train loss] ==> 0.0002\n",
      "[31/50] 0.003622319213502037\n",
      "[Train loss] ==> 0.0002\n",
      "[32/50] 0.0036008451476975878\n",
      "[Train loss] ==> 0.0002\n",
      "[33/50] 0.00355876042747349\n",
      "[Train loss] ==> 0.0002\n",
      "[34/50] 0.0034415495762611195\n",
      "[Train loss] ==> 0.0002\n",
      "[35/50] 0.003317717826575972\n",
      "[Train loss] ==> 0.0002\n",
      "[36/50] 0.0031868165870470137\n",
      "[Train loss] ==> 0.0002\n",
      "[37/50] 0.0032932441342944955\n",
      "[Train loss] ==> 0.0002\n",
      "[38/50] 0.0030989807574293253\n",
      "[Train loss] ==> 0.0002\n",
      "[39/50] 0.0032621644647969376\n",
      "[Train loss] ==> 0.0002\n",
      "[40/50] 0.0032152059376953515\n",
      "[Train loss] ==> 0.0002\n",
      "[41/50] 0.0029175300655229614\n",
      "[Train loss] ==> 0.0001\n",
      "[42/50] 0.0030883737870767832\n",
      "[Train loss] ==> 0.0002\n",
      "[43/50] 0.0028720263933445537\n",
      "[Train loss] ==> 0.0001\n",
      "[44/50] 0.0029709623106880046\n",
      "[Train loss] ==> 0.0001\n",
      "[45/50] 0.002794971051982776\n",
      "[Train loss] ==> 0.0001\n",
      "[46/50] 0.002913996442137951\n",
      "[Train loss] ==> 0.0001\n",
      "[47/50] 0.0029070430201023727\n",
      "[Train loss] ==> 0.0001\n",
      "[48/50] 0.0027979144235273238\n",
      "[Train loss] ==> 0.0001\n",
      "[49/50] 0.0026831535131870104\n",
      "[Train loss] ==> 0.0001\n",
      "[50/50] 0.0025673621887021695\n"
     ]
    }
   ],
   "source": [
    "for eps in range(EPOCHS + 1):\n",
    "    # 학습\n",
    "    train_loss = training()\n",
    "    # 검증\n",
    "    # testing()\n",
    "    print(f\"[{eps}/{EPOCHS}] {sum(train_loss)/len(train_loss)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Train_loss: 1.0349 Val_loss: 1.0406\n",
      "Epoch   10/1000 Train_loss: 0.2642 Val_loss: 0.3417\n",
      "Epoch   20/1000 Train_loss: 0.0836 Val_loss: 0.1914\n",
      "Epoch   30/1000 Train_loss: 0.0389 Val_loss: 0.1445\n",
      "Epoch   40/1000 Train_loss: 0.0223 Val_loss: 0.1196\n",
      "Epoch   50/1000 Train_loss: 0.0144 Val_loss: 0.1036\n",
      "Epoch   60/1000 Train_loss: 0.0101 Val_loss: 0.0922\n",
      "Epoch   70/1000 Train_loss: 0.0073 Val_loss: 0.0834\n",
      "Epoch   80/1000 Train_loss: 0.0055 Val_loss: 0.0761\n",
      "Epoch   90/1000 Train_loss: 0.0041 Val_loss: 0.0692\n",
      "Epoch  100/1000 Train_loss: 0.0032 Val_loss: 0.0641\n",
      "Epoch  110/1000 Train_loss: 0.0025 Val_loss: 0.0583\n",
      "Epoch  120/1000 Train_loss: 0.0019 Val_loss: 0.0537\n",
      "Epoch  130/1000 Train_loss: 0.0013 Val_loss: 0.0476\n",
      "Epoch  140/1000 Train_loss: 0.0012 Val_loss: 0.0454\n",
      "Epoch  150/1000 Train_loss: 0.0009 Val_loss: 0.0416\n",
      "Epoch  160/1000 Train_loss: 0.0008 Val_loss: 0.0389\n",
      "Epoch  170/1000 Train_loss: 0.0006 Val_loss: 0.0352\n",
      "Epoch  180/1000 Train_loss: 0.0005 Val_loss: 0.0331\n",
      "Epoch  190/1000 Train_loss: 0.0004 Val_loss: 0.0290\n",
      "Epoch  200/1000 Train_loss: 0.0003 Val_loss: 0.0260\n",
      "Epoch  210/1000 Train_loss: 0.0002 Val_loss: 0.0249\n",
      "Epoch  220/1000 Train_loss: 0.0002 Val_loss: 0.0212\n",
      "Epoch  230/1000 Train_loss: 0.0002 Val_loss: 0.0208\n",
      "Epoch  240/1000 Train_loss: 0.0001 Val_loss: 0.0186\n",
      "Epoch  250/1000 Train_loss: 0.0001 Val_loss: 0.0161\n",
      "Epoch  260/1000 Train_loss: 0.0001 Val_loss: 0.0161\n",
      "Epoch  270/1000 Train_loss: 0.0001 Val_loss: 0.0149\n",
      "Epoch  280/1000 Train_loss: 0.0001 Val_loss: 0.0131\n",
      "Epoch  290/1000 Train_loss: 0.0000 Val_loss: 0.0126\n",
      "Epoch  300/1000 Train_loss: 0.0000 Val_loss: 0.0121\n",
      "Epoch  310/1000 Train_loss: 0.0000 Val_loss: 0.0107\n",
      "Epoch  320/1000 Train_loss: 0.0000 Val_loss: 0.0107\n",
      "Epoch  330/1000 Train_loss: 0.0000 Val_loss: 0.0090\n",
      "Epoch  340/1000 Train_loss: 0.0000 Val_loss: 0.0092\n",
      "Epoch  350/1000 Train_loss: 0.0000 Val_loss: 0.0088\n",
      "Epoch  360/1000 Train_loss: 0.0000 Val_loss: 0.0081\n",
      "Epoch  370/1000 Train_loss: 0.0000 Val_loss: 0.0073\n",
      "Epoch  380/1000 Train_loss: 0.0000 Val_loss: 0.0066\n",
      "Epoch  390/1000 Train_loss: 0.0000 Val_loss: 0.0068\n",
      "Epoch  400/1000 Train_loss: 0.0000 Val_loss: 0.0061\n",
      "Epoch  410/1000 Train_loss: 0.0000 Val_loss: 0.0060\n",
      "Epoch  420/1000 Train_loss: 0.0000 Val_loss: 0.0058\n",
      "Epoch  430/1000 Train_loss: 0.0000 Val_loss: 0.0049\n",
      "Epoch  440/1000 Train_loss: 0.0000 Val_loss: 0.0050\n",
      "Epoch  450/1000 Train_loss: 0.0000 Val_loss: 0.0048\n",
      "Epoch  460/1000 Train_loss: 0.0000 Val_loss: 0.0040\n",
      "Epoch  470/1000 Train_loss: 0.0000 Val_loss: 0.0039\n",
      "Epoch  480/1000 Train_loss: 0.0000 Val_loss: 0.0037\n",
      "Epoch  490/1000 Train_loss: 0.0000 Val_loss: 0.0035\n",
      "Epoch  500/1000 Train_loss: 0.0000 Val_loss: 0.0034\n",
      "Epoch  510/1000 Train_loss: 0.0000 Val_loss: 0.0032\n",
      "Epoch  520/1000 Train_loss: 0.0000 Val_loss: 0.0031\n",
      "Epoch  530/1000 Train_loss: 0.0000 Val_loss: 0.0032\n",
      "Epoch  540/1000 Train_loss: 0.0000 Val_loss: 0.0030\n",
      "Epoch  550/1000 Train_loss: 0.0000 Val_loss: 0.0029\n",
      "Epoch  560/1000 Train_loss: 0.0000 Val_loss: 0.0027\n",
      "Epoch  570/1000 Train_loss: 0.0000 Val_loss: 0.0026\n",
      "Epoch  580/1000 Train_loss: 0.0000 Val_loss: 0.0025\n",
      "Epoch  590/1000 Train_loss: 0.0000 Val_loss: 0.0024\n",
      "Epoch  600/1000 Train_loss: 0.0000 Val_loss: 0.0023\n",
      "Epoch  610/1000 Train_loss: 0.0000 Val_loss: 0.0023\n",
      "Epoch  620/1000 Train_loss: 0.0000 Val_loss: 0.0021\n",
      "Epoch  630/1000 Train_loss: 0.0000 Val_loss: 0.0021\n",
      "Epoch  640/1000 Train_loss: 0.0000 Val_loss: 0.0020\n",
      "Epoch  650/1000 Train_loss: 0.0000 Val_loss: 0.0020\n",
      "Epoch  660/1000 Train_loss: 0.0000 Val_loss: 0.0019\n",
      "Epoch  670/1000 Train_loss: 0.0000 Val_loss: 0.0018\n",
      "Epoch  680/1000 Train_loss: 0.0000 Val_loss: 0.0018\n",
      "Epoch  690/1000 Train_loss: 0.0000 Val_loss: 0.0018\n",
      "Epoch  700/1000 Train_loss: 0.0000 Val_loss: 0.0017\n",
      "Epoch  710/1000 Train_loss: 0.0000 Val_loss: 0.0017\n",
      "Epoch  720/1000 Train_loss: 0.0000 Val_loss: 0.0017\n",
      "Epoch  730/1000 Train_loss: 0.0000 Val_loss: 0.0016\n",
      "Epoch  740/1000 Train_loss: 0.0000 Val_loss: 0.0015\n",
      "Epoch  750/1000 Train_loss: 0.0000 Val_loss: 0.0015\n",
      "Epoch  760/1000 Train_loss: 0.0000 Val_loss: 0.0015\n",
      "Epoch  770/1000 Train_loss: 0.0000 Val_loss: 0.0014\n",
      "Epoch  780/1000 Train_loss: 0.0000 Val_loss: 0.0014\n",
      "Epoch  790/1000 Train_loss: 0.0000 Val_loss: 0.0014\n",
      "Epoch  800/1000 Train_loss: 0.0000 Val_loss: 0.0014\n",
      "Epoch  810/1000 Train_loss: 0.0000 Val_loss: 0.0013\n",
      "Epoch  820/1000 Train_loss: 0.0000 Val_loss: 0.0014\n",
      "Epoch  830/1000 Train_loss: 0.0000 Val_loss: 0.0013\n",
      "Epoch  840/1000 Train_loss: 0.0000 Val_loss: 0.0013\n",
      "Epoch  850/1000 Train_loss: 0.0000 Val_loss: 0.0013\n",
      "Epoch  860/1000 Train_loss: 0.0000 Val_loss: 0.0012\n",
      "Epoch  870/1000 Train_loss: 0.0000 Val_loss: 0.0011\n",
      "Epoch  880/1000 Train_loss: 0.0000 Val_loss: 0.0012\n",
      "Epoch  890/1000 Train_loss: 0.0000 Val_loss: 0.0012\n",
      "Epoch  900/1000 Train_loss: 0.0000 Val_loss: 0.0011\n",
      "Epoch  910/1000 Train_loss: 0.0000 Val_loss: 0.0011\n",
      "Epoch  920/1000 Train_loss: 0.0000 Val_loss: 0.0011\n",
      "Epoch  930/1000 Train_loss: 0.0000 Val_loss: 0.0011\n",
      "Epoch  940/1000 Train_loss: 0.0000 Val_loss: 0.0011\n",
      "Epoch  950/1000 Train_loss: 0.0000 Val_loss: 0.0011\n",
      "Epoch  960/1000 Train_loss: 0.0000 Val_loss: 0.0010\n",
      "Epoch  970/1000 Train_loss: 0.0000 Val_loss: 0.0010\n",
      "Epoch  980/1000 Train_loss: 0.0000 Val_loss: 0.0010\n",
      "Epoch  990/1000 Train_loss: 0.0000 Val_loss: 0.0010\n",
      "Epoch 1000/1000 Train_loss: 0.0000 Val_loss: 0.0010\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1000\n",
    "for epoch in range(EPOCHS + 1):\n",
    "    for feature, target in trainDL:\n",
    "        prediction = model(feature.float())\n",
    "        train_loss = F.cross_entropy(prediction, target.squeeze().long())\n",
    "        OPTIMIZER.zero_grad()\n",
    "        train_loss.backward()\n",
    "        OPTIMIZER.step()\n",
    "    with torch.no_grad():\n",
    "        for feature_val, target_val in validDL:\n",
    "            pre_val = model(feature_val.float())\n",
    "            val_loss = F.cross_entropy(pre_val, target_val.squeeze().long())\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch:4d}/{EPOCHS} Train_loss: {train_loss.item():.6f} Val_loss: {val_loss.item():.6f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] feature tensor([[4.4000, 3.2000, 1.3000, 0.2000],\n",
      "        [5.7000, 3.0000, 4.2000, 1.2000],\n",
      "        [4.8000, 3.1000, 1.6000, 0.2000],\n",
      "        [5.6000, 2.9000, 3.6000, 1.3000],\n",
      "        [6.9000, 3.1000, 4.9000, 1.5000]], dtype=torch.float64) \n",
      "target tensor([[0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1]], dtype=torch.int32)\n",
      "[1] feature tensor([[5.0000, 3.2000, 1.2000, 0.2000],\n",
      "        [7.4000, 2.8000, 6.1000, 1.9000],\n",
      "        [5.0000, 3.5000, 1.3000, 0.3000],\n",
      "        [5.8000, 2.7000, 3.9000, 1.2000],\n",
      "        [5.1000, 3.5000, 1.4000, 0.3000]], dtype=torch.float64) \n",
      "target tensor([[0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]], dtype=torch.int32)\n",
      "[2] feature tensor([[6.7000, 2.5000, 5.8000, 1.8000],\n",
      "        [5.6000, 2.7000, 4.2000, 1.3000],\n",
      "        [6.2000, 2.2000, 4.5000, 1.5000],\n",
      "        [6.2000, 2.9000, 4.3000, 1.3000],\n",
      "        [7.7000, 3.8000, 6.7000, 2.2000]], dtype=torch.float64) \n",
      "target tensor([[2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2]], dtype=torch.int32)\n",
      "[3] feature tensor([[6.1000, 3.0000, 4.9000, 1.8000],\n",
      "        [4.5000, 2.3000, 1.3000, 0.3000],\n",
      "        [5.1000, 3.8000, 1.9000, 0.4000],\n",
      "        [4.9000, 2.4000, 3.3000, 1.0000],\n",
      "        [6.7000, 3.1000, 5.6000, 2.4000]], dtype=torch.float64) \n",
      "target tensor([[2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2]], dtype=torch.int32)\n",
      "[4] feature tensor([[5.9000, 3.0000, 5.1000, 1.8000],\n",
      "        [5.2000, 4.1000, 1.5000, 0.1000],\n",
      "        [5.1000, 3.3000, 1.7000, 0.5000],\n",
      "        [7.1000, 3.0000, 5.9000, 2.1000],\n",
      "        [5.4000, 3.9000, 1.3000, 0.4000]], dtype=torch.float64) \n",
      "target tensor([[2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0]], dtype=torch.int32)\n",
      "[5] feature tensor([[5.7000, 2.5000, 5.0000, 2.0000],\n",
      "        [6.1000, 2.8000, 4.0000, 1.3000],\n",
      "        [5.7000, 3.8000, 1.7000, 0.3000],\n",
      "        [5.8000, 2.7000, 4.1000, 1.0000],\n",
      "        [5.6000, 3.0000, 4.5000, 1.5000]], dtype=torch.float64) \n",
      "target tensor([[2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1]], dtype=torch.int32)\n",
      "[6] feature tensor([[5.1000, 3.5000, 1.4000, 0.2000],\n",
      "        [5.0000, 3.0000, 1.6000, 0.2000],\n",
      "        [5.8000, 2.7000, 5.1000, 1.9000],\n",
      "        [6.8000, 3.0000, 5.5000, 2.1000],\n",
      "        [6.1000, 3.0000, 4.6000, 1.4000]], dtype=torch.float64) \n",
      "target tensor([[0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], dtype=torch.int32)\n",
      "[7] feature tensor([[4.6000, 3.1000, 1.5000, 0.2000],\n",
      "        [5.2000, 2.7000, 3.9000, 1.4000],\n",
      "        [6.5000, 3.0000, 5.5000, 1.8000],\n",
      "        [6.7000, 3.1000, 4.7000, 1.5000],\n",
      "        [5.4000, 3.0000, 4.5000, 1.5000]], dtype=torch.float64) \n",
      "target tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1]], dtype=torch.int32)\n",
      "[8] feature tensor([[4.9000, 2.5000, 4.5000, 1.7000],\n",
      "        [5.8000, 2.7000, 5.1000, 1.9000],\n",
      "        [5.0000, 3.5000, 1.6000, 0.6000],\n",
      "        [5.1000, 3.4000, 1.5000, 0.2000],\n",
      "        [5.0000, 3.4000, 1.6000, 0.4000]], dtype=torch.float64) \n",
      "target tensor([[2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]], dtype=torch.int32)\n",
      "[9] feature tensor([[5.1000, 2.5000, 3.0000, 1.1000],\n",
      "        [5.0000, 2.3000, 3.3000, 1.0000],\n",
      "        [5.4000, 3.4000, 1.7000, 0.2000],\n",
      "        [6.3000, 2.3000, 4.4000, 1.3000],\n",
      "        [5.1000, 3.8000, 1.5000, 0.3000]], dtype=torch.float64) \n",
      "target tensor([[1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]], dtype=torch.int32)\n",
      "[10] feature tensor([[6.9000, 3.2000, 5.7000, 2.3000],\n",
      "        [5.8000, 2.8000, 5.1000, 2.4000],\n",
      "        [5.0000, 3.4000, 1.5000, 0.2000],\n",
      "        [6.1000, 2.9000, 4.7000, 1.4000],\n",
      "        [6.8000, 2.8000, 4.8000, 1.4000]], dtype=torch.float64) \n",
      "target tensor([[2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1]], dtype=torch.int32)\n",
      "[11] feature tensor([[5.5000, 2.5000, 4.0000, 1.3000],\n",
      "        [5.5000, 3.5000, 1.3000, 0.2000],\n",
      "        [4.8000, 3.0000, 1.4000, 0.3000],\n",
      "        [4.9000, 3.1000, 1.5000, 0.1000],\n",
      "        [6.3000, 3.3000, 4.7000, 1.6000]], dtype=torch.float64) \n",
      "target tensor([[1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1]], dtype=torch.int32)\n",
      "[12] feature tensor([[6.6000, 2.9000, 4.6000, 1.3000],\n",
      "        [7.7000, 2.8000, 6.7000, 2.0000],\n",
      "        [6.4000, 3.2000, 4.5000, 1.5000],\n",
      "        [6.7000, 3.0000, 5.2000, 2.3000],\n",
      "        [4.8000, 3.4000, 1.9000, 0.2000]], dtype=torch.float64) \n",
      "target tensor([[1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0]], dtype=torch.int32)\n",
      "[13] feature tensor([[5.1000, 3.7000, 1.5000, 0.4000],\n",
      "        [7.6000, 3.0000, 6.6000, 2.1000],\n",
      "        [6.0000, 2.2000, 4.0000, 1.0000],\n",
      "        [5.7000, 4.4000, 1.5000, 0.4000],\n",
      "        [4.8000, 3.4000, 1.6000, 0.2000]], dtype=torch.float64) \n",
      "target tensor([[0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]], dtype=torch.int32)\n",
      "[14] feature tensor([[5.3000, 3.7000, 1.5000, 0.2000],\n",
      "        [6.3000, 2.8000, 5.1000, 1.5000],\n",
      "        [5.6000, 3.0000, 4.1000, 1.3000],\n",
      "        [7.0000, 3.2000, 4.7000, 1.4000],\n",
      "        [4.6000, 3.4000, 1.4000, 0.3000]], dtype=torch.float64) \n",
      "target tensor([[0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0]], dtype=torch.int32)\n",
      "[15] feature tensor([[6.1000, 2.6000, 5.6000, 1.4000],\n",
      "        [6.4000, 2.7000, 5.3000, 1.9000],\n",
      "        [4.4000, 2.9000, 1.4000, 0.2000],\n",
      "        [5.0000, 3.3000, 1.4000, 0.2000],\n",
      "        [6.6000, 3.0000, 4.4000, 1.4000]], dtype=torch.float64) \n",
      "target tensor([[2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1]], dtype=torch.int32)\n",
      "[16] feature tensor([[5.6000, 2.5000, 3.9000, 1.1000],\n",
      "        [6.7000, 3.3000, 5.7000, 2.1000],\n",
      "        [5.0000, 3.6000, 1.4000, 0.2000],\n",
      "        [6.5000, 3.0000, 5.2000, 2.0000],\n",
      "        [5.5000, 2.4000, 3.8000, 1.1000]], dtype=torch.float64) \n",
      "target tensor([[1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1]], dtype=torch.int32)\n",
      "[17] feature tensor([[6.3000, 3.3000, 6.0000, 2.5000],\n",
      "        [5.7000, 2.8000, 4.1000, 1.3000],\n",
      "        [6.9000, 3.1000, 5.1000, 2.3000],\n",
      "        [4.6000, 3.2000, 1.4000, 0.2000],\n",
      "        [7.3000, 2.9000, 6.3000, 1.8000]], dtype=torch.float64) \n",
      "target tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2]], dtype=torch.int32)\n",
      "[18] feature tensor([[4.3000, 3.0000, 1.1000, 0.1000],\n",
      "        [7.2000, 3.6000, 6.1000, 2.5000],\n",
      "        [7.2000, 3.0000, 5.8000, 1.6000],\n",
      "        [5.2000, 3.4000, 1.4000, 0.2000],\n",
      "        [4.4000, 3.0000, 1.3000, 0.2000]], dtype=torch.float64) \n",
      "target tensor([[0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0]], dtype=torch.int32)\n",
      "[19] feature tensor([[5.5000, 2.3000, 4.0000, 1.3000],\n",
      "        [5.6000, 2.8000, 4.9000, 2.0000],\n",
      "        [5.4000, 3.9000, 1.7000, 0.4000],\n",
      "        [5.7000, 2.8000, 4.5000, 1.3000],\n",
      "        [5.4000, 3.4000, 1.5000, 0.4000]], dtype=torch.float64) \n",
      "target tensor([[1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]], dtype=torch.int32)\n",
      "[20] feature tensor([[6.1000, 2.8000, 4.7000, 1.2000],\n",
      "        [6.4000, 2.9000, 4.3000, 1.3000],\n",
      "        [6.5000, 2.8000, 4.6000, 1.5000],\n",
      "        [4.7000, 3.2000, 1.6000, 0.2000],\n",
      "        [4.8000, 3.0000, 1.4000, 0.1000]], dtype=torch.float64) \n",
      "target tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# DataLoader 속성\n",
    "for _, (feature, target) in enumerate(trainDL):\n",
    "    print(f\"[{_}] feature {feature} \\ntarget {target}\")\n",
    "    ## 로더에서 가지고 온 데이터 만큼 학습 진행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_PY38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
